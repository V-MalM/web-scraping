{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from splinter import Browser\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from webdriver_manager.chrome import ChromeDriverManager\r\n",
    "import time\r\n",
    "import pandas as pd    \r\n",
    "\r\n",
    "def scrape(): \r\n",
    "\r\n",
    "    # Setup splinter\r\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\r\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\r\n",
    "\r\n",
    "    scrape_data = {}\r\n",
    "\r\n",
    "    #### NASA Mars News\r\n",
    "\r\n",
    "\r\n",
    "    # * Scrape the Mars News Site and collect the latest News Title and Paragraph Text. \r\n",
    "    # Assign the text to variables that you can reference later.\r\n",
    "\r\n",
    "    url_redplanetscience = 'https://redplanetscience.com'\r\n",
    "    browser.visit(url_redplanetscience)\r\n",
    "    time.sleep(2)\r\n",
    "    #browser.reload()\r\n",
    "\r\n",
    "    html_redplanetscience = browser.html\r\n",
    "    soup_redplanetscience = BeautifulSoup(html_redplanetscience, 'html.parser')\r\n",
    "\r\n",
    "    results_redplanetscience = soup_redplanetscience.find('div', class_='list_text')\r\n",
    "    # results = soup.find_all('div', attrs={'class': ['list_date','content_title','article_teaser_body']})\r\n",
    "    # print(results)\r\n",
    "    news_date  = results_redplanetscience.find('div', class_='list_date').text\r\n",
    "    news_title = results_redplanetscience.find('div', class_='content_title').text\r\n",
    "    news_para  = results_redplanetscience.find('div', class_='article_teaser_body').text\r\n",
    "\r\n",
    "    # print (f\"{news_date} :\\n{news_title}\\n{news_para}\")\r\n",
    "    scrape_data[\"news_date\"]  = news_date\r\n",
    "    scrape_data[\"news_title\"] = news_title\r\n",
    "    scrape_data[\"news_para\"]  = news_para\r\n",
    "\r\n",
    "\r\n",
    "    #### JPL Mars Space Images - Featured Image\r\n",
    "    # * Visit the url for the Featured Space Image site \"https://spaceimages-mars.com\".\r\n",
    "    # * Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\r\n",
    "    # * Make sure to find the image url to the full size .jpg image.\r\n",
    "    # * Make sure to save a complete url string for this image.\r\n",
    "\r\n",
    "\r\n",
    "    url_spaceimages = 'https://spaceimages-mars.com/'\r\n",
    "    browser.visit(url_spaceimages)\r\n",
    "    browser.reload()\r\n",
    "    html_spaceimages = browser.html\r\n",
    "    soup_spaceimages = BeautifulSoup(html_spaceimages, 'html.parser')\r\n",
    "\r\n",
    "    results_spaceimages = soup_spaceimages.find('a', class_='showimg fancybox-thumbs')\r\n",
    "    # print(results_spaceimages)\r\n",
    "    featured_image = results_spaceimages.get('href')\r\n",
    "    featured_image_url = url_spaceimages + featured_image\r\n",
    "    # print(featured_image_url)\r\n",
    "    scrape_data[\"featured_image_url\"]  = featured_image_url\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    #### Mars Facts\r\n",
    "    # * Visit the Mars Facts webpage 'https://galaxyfacts-mars.com' \r\n",
    "    # and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\r\n",
    "    # * Use Pandas to convert the data to a HTML table string.\r\n",
    "\r\n",
    "    url_galaxyfacts = 'https://galaxyfacts-mars.com'\r\n",
    "    tables_galaxyfacts = pd.read_html(url_galaxyfacts)\r\n",
    "\r\n",
    "    # print(tables_galaxyfacts)\r\n",
    "\r\n",
    "    df = tables_galaxyfacts[1]\r\n",
    "    galaxyfacts_html = df.to_html()\r\n",
    "    galaxyfacts_html = galaxyfacts_html.replace('\\n','')\r\n",
    "    #print(galaxyfacts_html)\r\n",
    "    scrape_data[\"galaxyfacts_html\"]  = galaxyfacts_html\r\n",
    "\r\n",
    "    ### Mars Hemispheres\r\n",
    "\r\n",
    "    # * Visit the astrogeology site 'https://marshemispheres.com/' to obtain high resolution images for each of Mar's hemispheres.\r\n",
    "\r\n",
    "    # * You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\r\n",
    "\r\n",
    "    # * Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\r\n",
    "\r\n",
    "    # * Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\r\n",
    "\r\n",
    "    # ```python\r\n",
    "    # # Example:\r\n",
    "    # hemisphere_image_urls = [\r\n",
    "    #     {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\r\n",
    "    #     {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\r\n",
    "    #     {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\r\n",
    "    #     {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\r\n",
    "    # ]\r\n",
    "    # ```\r\n",
    "\r\n",
    "    url_hem = 'https://marshemispheres.com/'\r\n",
    "    browser.visit(url_hem)\r\n",
    "    #browser.reload()\r\n",
    "    hemisphere_image_urls  = []\r\n",
    "    url_hem_html = browser.html\r\n",
    "\r\n",
    "    soup_url_hem_html = BeautifulSoup(url_hem_html,'html.parser')\r\n",
    "    hem_img_divs = soup_url_hem_html.find_all('div', class_=\"description\")\r\n",
    "    for item in hem_img_divs:\r\n",
    "        #print (item.find('a').get('href'))\r\n",
    "        link_text = item.find('a').text\r\n",
    "        link_text=link_text.strip()\r\n",
    "        \r\n",
    "        but = browser.find_by_text(link_text)\r\n",
    "        but.click()\r\n",
    "\r\n",
    "        # At this point broswer has new url\r\n",
    "        soup_full_res = BeautifulSoup(browser.html,'html.parser')\r\n",
    "        full_res_div  = soup_full_res.find_all('div', class_=\"downloads\")\r\n",
    "        full_res_img_url = url_hem + full_res_div[0].find('a').get('href')\r\n",
    "        # print(full_res_img_url)\r\n",
    "        \r\n",
    "        url_dict = {\"title\":link_text, \"img_url=\":full_res_img_url}\r\n",
    "        hemisphere_image_urls.append(url_dict)\r\n",
    "\r\n",
    "        browser.visit(url_hem)\r\n",
    "        #browser.reload()\r\n",
    "\r\n",
    "    # print(hemisphere_image_urls)\r\n",
    "    scrape_data[\"hemisphere_image_urls\"]  = hemisphere_image_urls\r\n",
    "\r\n",
    "    browser.quit()\r\n",
    "\r\n",
    "    return scrape_data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "sd = scrape()\r\n",
    "print(sd)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\vmuty\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-30e8c0a1a04a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-30e8c0a1a04a>\u001b[0m in \u001b[0;36mscrape\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# results = soup.find_all('div', attrs={'class': ['list_date','content_title','article_teaser_body']})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# print(results)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mnews_date\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mresults_redplanetscience\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'list_date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mnews_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_redplanetscience\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'content_title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mnews_para\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mresults_redplanetscience\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'article_teaser_body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "eb0a5deb85624cf39522bd9ea766efe3b585b85d5e69ded4a98e5c69611b9570"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}